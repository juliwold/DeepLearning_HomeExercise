---
title: "Home excericse - NOVA course on deep learning in remote sensing"
author: "Julius Wold"
date: "2023-06-29"
format: gfm
---

```{python}
#| label: python-packages
#| include: FALSE

import os
from glob import glob
import itertools
import json

import pandas as pd
```

```{r}
#| label: r-packages
#| include: FALSE

library(here)
library(tidyverse)
library(sf)
library(tmap)
library(reticulate)
```

Code used for experiment can be found in [this repository](<https://github.com/juliwold/DeepLearning_HomeExercise>).

# Objective

Compare the effect of training a seedling detector on your own annotated dataset vs. the full dataset on the detectorâ€™s performance.

# Materials and methods

## Annotations

Two sets of annotations were used for model tranining: **My annotations** and **All annotations**.

- **My annotations**
  - Annotations made by the author.
- **All annotations**
  - Annotations from all students merged.

**My annotations** consists of 48 images with 280 annotations of trees while **All annotations** consists of 387 images with 5062 annotations. 70% of data was randomly assigned to training and 30% to validation. An overview of images and annotations used in the tranining and validation split are shown in @tbl-training.

```{python}
#| label: collect-data-summary
#| include: FALSE

def number_of_images(anno_data_path):
    n_train = len(glob(os.path.join(anno_data_path, "train", "images", "*.tif")))
    n_val = len(glob(os.path.join(anno_data_path, "val", "images", "*.tif")))
    n_images = {"train": n_train, "val": n_val, "all": n_train + n_val}

    return n_images


def number_of_boxes(anno_data_path):
    labels_train = glob(os.path.join(anno_data_path, "train", "labels", "*.txt"))
    n_train = sum(map(count_boxes, labels_train))
    labels_val = glob(os.path.join(anno_data_path, "val", "labels", "*.txt"))
    n_val = sum(map(count_boxes, labels_val))
    n_boxes = {"train": n_train, "val": n_val, "all": n_train + n_val}

    return n_boxes


def count_boxes(label):
    with open(label, "r") as f:
        n_boxes = len(f.readlines())

    return n_boxes


def anno_summary(anno_data_path):
    annotations_set = os.path.basename(anno_data_path).replace("_", " ").capitalize()
    n_images = number_of_images(anno_data_path)
    n_boxes = number_of_boxes(anno_data_path)
    df_index = pd.MultiIndex.from_product([["Number of images", "Number of trees"], ["Train", "Val", "Sum"]], names=["Count", "Split"])
    summary = pd.DataFrame({annotations_set: list(n_images.values()) + list(n_boxes.values())}, index=df_index)

    return summary


annotations = glob(os.path.join("..", "data", "annotated_data", "train", "*"))
summary = pd.concat(map(anno_summary, annotations), axis=1)
summary_index = summary.index.to_frame()
```

```{r}
#| echo: FALSE
#| label: tbl-training
#| tbl-cap: "Number of images and annotatations for each dataset."

summary_table <-
    bind_cols(py$summary_index, py$summary) %>%
    knitr::kable()
summary_table
```

## Test data

Two sets of test data was used for evaluating models: **Tiled test data** and **Drone RGB orthomosaics**.

- **Tiled test data**
  - Used for ML metrics evaluation.
  - Tiled and annotated images.
- **Drone RGB orthomosaics**
  - Used for domain metrics evaluation.
  - Orthomosaics of four sites.
    - Each site contains four plots (~0.1 ha) with tree posistions measured in field.

## Model training

YOLOv8 models were trained using the dataset **My annotations** and the dataset **All annotations**. A grid search were performed for model sizes *Nano*, *Medium* and *Xtra large* and image sizes *256*, *640* and *1024* (@tbl-search). The best model for each dataset were selected using mAP@.5.

```{python}
#| label: grid-search
#| include: FALSE

grid_search = pd.DataFrame(list(itertools.product(["yolov8n.pt", "yolov8m.pt", "yolov8x.pt"], [256, 640, 1024])), columns=["Model", "Image size"])
```

```{r}
#| echo: FALSE
#| label: tbl-search
#| tbl-cap: Grid search.

knitr::kable(py$grid_search)
```

## Model evaluation

The selected models from the model training were evaluated using machine-learning metris and domain metrics on the test data.

### ML metrics

ML metrics were evaluated by testing the models against the **tiled test data** using the inbuilt [val mode](<https://docs.ultralytics.com/modes/val/>) at default settings. 

### Domain metrics

Residual Mean Square Error (RMSE) and Mean Deviation (MD) were calculated according to @eq-rmse and @eq-md for trees per ha. Additionally RMSE (%) and MD (%) were calculated as size of RMSE and MD relative to the observed mean.

$$
RMSE = \sqrt{\frac{\Sigma_{i=1}^{n}(\hat{y_i} - y_i)^2}{n}}
$$ {#eq-rmse}

$$
MD = \frac{\Sigma_{i=1}^{n}(\hat{y_i} - y_i)}{n}
$$ {#eq-md}

# Results & Discussion

## Model training

> Detailed view of trained models can be found in [this Comet project](<https://www.comet.com/juliwold/home-exercise-sapling-detector/view/uxR2erf0uJlERPXPjybwdN2yE/panels>).

### Training performance

Models trained by the **My annotations** dataset achived better performance in training than the models trained on **All annotations**. For **My annotations** the highest performance was achived by the *nano* model with image size *640* (mAP@.5 of 0.59 at epoch 94). The best perfoming model for **All annotations** was the *meduim* model with an image size of *640* (mAP@.5 of 0.36 at epoch 42). Summary of training results for are shown in @tbl-trainingperformance-1 and @tbl-trainingperformance-2 for **My annotations** and **All annotations** respectively.

```{r}
#| label: collect-traning-performance
#| include: FALSE

my_perfomance <-
    here("models", "model_performance", "juliwold_home_exercise_sapling_detector_sapling_detector_table_data_my_annotations.csv") %>%
    read_csv(show_col_types = FALSE) %>%
    select(-data)
all_perfomance <-
    here("models", "model_performance", "juliwold_home_exercise_sapling_detector_sapling_detector_table_data_all_annotations.csv") %>%
    read_csv(show_col_types = FALSE) %>%
    select(-data)
```

```{r}
#| echo: FALSE
#| label: tbl-trainingperformance
#| tbl-cap: "Training performance of best models."
#| tbl-subcap:
#|   - "My annotations"
#|   - "All annotations"
#| layout-ncol: 1
training_perfomance <- function(x) {
    x %>%
        select(-Name) %>%
        knitr::kable(digits = 3)
}

training_perfomance(my_perfomance)
training_perfomance(all_perfomance)
```

There was little impact on training perfomance with varying image size and model size for models trained with **All annotations**. For models trained with **My annotations** a difference can be observed between *nano* models and *medium* and *xtra large* models. *Medium* and *xtra large* seems to improve faster than *nano* models (higher performance at earlier epoch) but starts to overfit earlier on the data. mAP@.5 curves for models are shown in @fig-training.

It is suprising that *xtra large* models did not perform better than the smaller model sizes, low quality of annotations or the small size of the dataset might be reasons for this.

```{r}
#| label: collect-training-profiles
#| include: FALSE
collect_training_results <- function(model_path) {
    training_results <-
        here(model_path, "results.csv") %>%
        read_csv(show_col_types = FALSE) %>%
        mutate(model = basename(model_path), .before = 1)
}
training_results <-
    map(
        c("my_annotations", "all_annotations"),
        .f = ~ {
            results_annotations <-
                here("models", .x) %>%
                list.files(full.names = TRUE) %>%
                map(collect_training_results) %>%
                bind_rows() %>%
                mutate(
                    annotation_set = .x, .before = 1,
                    model = str_remove(model, .x)
                )
        }
    ) %>%
    bind_rows()
```

```{r}
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| label: fig-training
#| fig-cap: Training results

training_plot <-
    training_results %>%
    mutate(
        model_size = str_extract(model, "yolov8[nmx]"),
        image_size = str_extract(model, "\\d{3,4}")
    ) %>%
    ggplot(aes(epoch, `metrics/mAP50(B)`, color = model_size, linetype = image_size)) +
    geom_line(alpha = 0.5) +
    geom_smooth(se = FALSE) +
    colorspace::scale_color_discrete_qualitative(palette = "Dark3") +
    # scale_colour_viridis_d() +
    facet_wrap(~annotation_set, ncol = 1, scales = "free")
training_plot
```

### Model speed

Increasing model size (*Nano* -> *Medium* -> *Xtra large*) resulted in slower models. The effect of image size on model speed was greater with increasing model size. Image size had little effect on *nano* models but lead to much slower models for *medium* and *xtra large* models. @fig-modelspeed shows the effect of model size and image size on model speed.

::: {#fig-modelspeed layout-ncol=2}

![My annotations](<https://github.com/juliwold/DeepLearning_HomeExercise/blob/main/figures/model_speed_my_annotations.jpeg>){#fig-modelspeed-my}
![All annotations](<https://github.com/juliwold/DeepLearning_HomeExercise/blob/main/figures/model_speed_all_annotations.jpeg>){#fig-modelspeed-all}


Model speed (size of dots represents image size).
:::

## Model evaluation

```{python}
#| label: selected-models
#| include: FALSE
models = [
    "..\\models\\my_annotations\\my_annotations_yolov8n.pt_640",
    "..\\models\\all_annotations\\all_annotations_yolov8m.pt_640",
]
```

### ML metrics

There was no large difference in performance between the models trained on **My annotations** or **All annotations**. **All annotations** performed slighty better with an mAP@.5 of 0.38 compared to 0.36 for **My annotations**. ML metrics of the models are shown in @tbl-mlmetrics.

```{python}
#| label: collect-ml-metrics
#| include: FALSE

def collect_ml_metrics(model_path):
    with open(os.path.join(model_path, "val", "test_results.json"), "r") as f:
        ml_metrics = json.load(f)
    metrics_table = pd.DataFrame(ml_metrics, index=[os.path.basename(model_path)])

    return(metrics_table)


ml_metrics = pd.concat(map(collect_ml_metrics, models)).round(decimals=2)
```

```{r}
#| echo: FALSE
#| label: tbl-mlmetrics
#| tbl-cap: Machine learnining metrics.

knitr::kable(py$ml_metrics)
```

### Domain metrics

RMSE and MD of predicted trees per. ha are shown in @tbl-domainmetrics and predicted and observed values are shown in @fig-op. The difference in domain metrics between models were also here quite low but here the model trained on **My annotations** performed better. RMSE and RMSE (%) over all sites were respectivley 623 trees/ha and 42% for **My annotations** and 632 trees/ha and 43% for **All annotations**. Both models struggled with detections on the site Braatan, with RMSE (%) of 74% and 67% respectivley for **My annotations** and *All annotations**. The best performance was found on site Hobol with RMSE (%) of 10% and 9%.

Both models consistently underestimated the number of trees on all sites. MD and MD (%) over all sites were respectivley -473 trees/ha and -32% for **My annotations** and -499 trees/ha and -34% for **All annotations**.

```{python}
#| label: collect-domain-metrics
#| include: FALSE

def collect_domain_metrics(model_path):
    metrics = pd.read_csv(os.path.join(model_path, "predictions_processed", "metrics.csv"))
    metrics["Model"] = os.path.basename(model_path)
    return(metrics)


dm_all = pd.concat(map(collect_domain_metrics, models))
mi = pd.MultiIndex.from_frame(dm_all[["Model", "aoi_name"]])

domain_metrics = dm_all.set_index(mi).round()
domain_metrics = domain_metrics.drop(["aoi_name", "Model", "rmse_n", "rmse_n (%)","bias_n", "bias_n (%)",], axis = 1)
domain_metrics = domain_metrics.rename(columns={"rmse_dens": "RMSE", "rmse_dens (%)": "RMSE (%)", "bias_dens": "MD", "bias_dens (%)": "MD (%)"})
domain_index = domain_metrics.index.to_frame()
```

```{r}
#| echo: FALSE
#| label: tbl-domainmetrics
#| tbl-cap: Domain metrics.
domain_table <-
    bind_cols(py$domain_index, py$domain_metrics) %>%
    knitr::kable()
domain_table
```

```{r}
#| echo: FALSE
#| label: fig-op
#| fig-cap: "Predicted vs. observed trees/ha. Both models underestimate the number of trees per. ha and struggle with predictions at the sites Braatan and Galbyveien."
domain_predictions <-
    map(
        c(
            "models\\my_annotations\\my_annotations_yolov8n.pt_640",
            "models\\all_annotations\\all_annotations_yolov8m.pt_640"
        ),
        .f = ~ {
            here(.x, "predictions_processed", "domain_predictions.csv") %>%
                read_sf() %>%
                mutate(
                    model = basename(.x), .before = 1,
                    reference_dens = as.numeric(reference_dens),
                    predicted_dens = as.numeric(predicted_dens)
                )
        }
    ) %>%
    bind_rows()

max_value <- max(c(domain_predictions$reference_dens, domain_predictions$predicted_dens))

op_plot <-
    ggplot(domain_predictions, aes(predicted_dens, reference_dens, color = aoi_name)) +
    geom_point() +
    geom_abline() +
    xlim(0, max_value) +
    ylim(0, max_value) +
    labs(x = "Predicted trees/ha", y = "Observed trees/ha") +
    scale_colour_viridis_d() +
    facet_wrap(~model, ncol = 1)
op_plot
```

## Examples of bad performance.

Both detectors performed poorly at the sites Braatan and Galbyveien. The first example is shown in @fig-braatan1, both detectors struggle with small saplings and trees clumped together. Examples shown in @fig-braatan2 and @fig-galbyveien both show problems with detection of small saplings.

Improving the annotation quality would likley be the best strategy for improving the performance of models, especially for the small saplings. Many of the small saplings are likley to be unnannotated since they are difficult to detect for unexperienced people, which will negativly effect the models performance on small saplings. Saplings of pine are probably affected more by this, as they are more difficult to detect in annotation.

Increasing the value of the hyperparameter *imgsz* could also be a good strategy of improving performance on small saplings, as the size of the saplings in the image is quite low. Increasing image size from the default had little effect in this experiment, poor quality of annotations on small saplings can be a cause for this.

Improving the performance of trees clustered togheter is probably more difficult since the models need to differentiate between clustered trees and windfallen trees. Improving annotation quality would likley be the best option.

```{r}
#| label: collect-preds-and-ref
#| include: FALSE
sites <-
    if (!file.exists(here("data", "map_data", "plots.geojson"))) {
        bind_rows(
            here("data", "map_data", "aois.geojson") %>%
                read_sf() %>%
                mutate(aoi_name = "galbyveien"),
            here("data", "map_data", "test_plots.geojson") %>%
                read_sf() %>%
                filter(!is.na(aoi_name), aoi_name != "NULL")
        ) %>%
            mutate(aoi_name = str_remove(aoi_name, "\\d+")) %>%
            group_by(aoi_name) %>%
            mutate(id = row_number()) %>%
            ungroup() %>%
            st_write(here("data", "map_data", "plots.geojson"))
    } else {
        st_read(here("data", "map_data", "plots.geojson"))
    }

predictions_my <-
    here("models", "my_annotations", "my_annotations_yolov8n.pt_640", "predictions_processed", "predictions.shp") %>%
    read_sf()
predictions_all <-
    here("models", "all_annotations", "all_annotations_yolov8m.pt_640", "predictions_processed", "predictions.shp") %>%
    read_sf()
reference <-
    here("data", "map_data", "test_annotations2_sun.geojson") %>% read_sf()
```

```{r}
#| label: plot-preds-func
#| include: FALSE
plot_preds_on_site <- function(site, preds, refs) {
    ortho <-
        here("data", "orthomosaics", "test_data") %>%
        list.files(pattern = ".tif$", full.names = TRUE) %>%
        keep(function(x) str_detect(x, site$aoi_name)) %>%
        terra::rast() %>%
        terra::crop(site)

    preds_on_site <-
        preds %>%
        filter(., as.logical(st_intersects(., site, sparse = FALSE)))
    refs_on_site <-
        refs %>%
        filter(., as.logical(st_intersects(., site, sparse = FALSE)))
    tm_shape(ortho) +
        tm_rgb() +
    tm_shape(refs_on_site) +
        tm_borders(col = "red", lwd = 2) +  # TODO: Adjust colors.
    tm_shape(preds_on_site) +
        tm_borders(col = "blue", lwd = 2)  # TODO: Adjust colors.
}
```

```{r}
#| include: FALSE
braatan_1_my <-
    sites %>%
    filter(aoi_name == "braatan", id == 3) %>%
    plot_preds_on_site(predictions_my, reference)
braatan_1_all <-
    sites %>%
    filter(aoi_name == "braatan", id == 3) %>%
    plot_preds_on_site(predictions_all, reference)
```

```{r}
#| echo: FALSE
#| warning: FALSE
#| label: fig-braatan1
#| fig-cap: "Poor detections at Braatan. Many larger trees close together missed by both detectors."
#| fig-subcap:
#|   - "My annotations"
#|   - "All annotations"
#| layout-ncol: 1

braatan_1_my
braatan_1_all
```

```{r}
#| include: FALSE
braatan_2_my <-
    sites %>%
    filter(aoi_name == "braatan", id == 2) %>%
    plot_preds_on_site(predictions_my, reference)
braatan_2_all <-
    sites %>%
    filter(aoi_name == "braatan", id == 2) %>%
    plot_preds_on_site(predictions_all, reference)
```

```{r}
#| echo: FALSE
#| warning: FALSE
#| label: fig-braatan2
#| fig-cap: "Poor detections at Braatan. Small saplings missed by both detectors (pine?)"
#| fig-subcap:
#|   - "My annotations"
#|   - "All annotations"
#| layout-ncol: 1

braatan_2_my
braatan_2_all
```

```{r}
#| include: FALSE
galbyveien_my <-
    sites %>%
    filter(aoi_name == "galbyveien", id == 1) %>%
    plot_preds_on_site(predictions_my, reference)
galbyveien_all <-
    sites %>%
    filter(aoi_name == "galbyveien", id == 1) %>%
    plot_preds_on_site(predictions_all, reference)
```

```{r}
#| echo: FALSE
#| warning: FALSE
#| label: fig-galbyveien
#| fig-cap: "Poor detection at site Galbyveien. Undetected smaller saplings."
#| fig-subcap:
#|   - "My annotations"
#|   - "All annotations"
#| layout-ncol: 1

galbyveien_my
galbyveien_all
```

## Comparison of models

There was little difference in performance between the two sets of annotations. **My annotations** consisting of 48 images with 280 annotaitons performed similar to **All annotations** consisting of 387 images with 5062 annotations. Higher quality of annotations in **My annotations** may compensate for the decrease in dataset size compared to **All annotations**. The larger dataset size of **All annotations** might also be the reason for the *medium* size model performing best, compared to the *nano* model selected for **My annotations**.

# Conclusion

- Quality of annotations is more important than quantity of annotations.
  - My set of 48 annotated images performed similar to the dataset of 387 images with annotations.
- Model size has the largest impact on model speed.
  - The effect of image size increases with model size.
- Training performance is not representative for domain performance.
  - **All annotations** performed worse than **My annotations** in training while the difference assessed by ML or domain metrics were minimal.
